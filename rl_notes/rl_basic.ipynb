{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZwxhRMoq16t+zmtCTx6fX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelwfc/notes/blob/main/rl_notes/rl_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The big picture\n",
        "\n",
        "The idea behind Reinforcement Learning is that an agent (an AI) will learn from the environment by interacting with it (through trial and error) and receiving rewards (negative or positive) as feedback for performing actions.\n",
        "\n",
        "Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.\n",
        "\n",
        "## The reward hypothesis: the central idea of Reinforcement Learning\n",
        "\n",
        "⇒ Why is the goal of the agent to **maximize the expected return**?\n",
        "\n",
        "Because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected return (expected cumulative reward).\n",
        "\n",
        "That’s why in Reinforcement Learning, to have the best behavior, we aim to learn to take actions that maximize the expected cumulative reward.\n",
        "\n",
        "**Rewards and the discounting**：The reward is fundamental in RL because it’s the only feedback for the agent. Thanks to it, our agent knows if the action taken was good or not.\n",
        "\n",
        "## The Policy π: the agent’s brain\n",
        "\n",
        "The Policy π is the brain of our Agent, it’s the function that tells us what action to take given the state we are in. So it defines the agent’s behavior at a given time.\n",
        "\n",
        "\n",
        "**the goal of an RL agent** is to have **an optimal policy π***：\n",
        "This Policy is the function we want to learn, our goal is to find the optimal policy π*, the policy that maximizes expected return when the agent acts according to it. We find this π* through training.\n",
        "\n",
        "\n",
        "There are two approaches to train our agent to find this optimal policy π*:\n",
        "\n",
        "\n",
        "# Two main approaches for solving RL problems\n",
        "## Policy-Based Methods\n",
        "  \n",
        "Directly, by teaching the agent to learn which action to take, given the current state. we learn a policy function directly.\n",
        "This function will define a mapping from each state to the best corresponding action. \n",
        "\n",
        "Alternatively, it could define a probability distribution over the set of possible actions at that state.\n",
        "\n",
        "\n",
        "## value-based methods\n",
        "  \n",
        "Indirectly, teach the agent to learn which state is more valuable and then take the action that leads to the more valuable states.\n",
        "  1. **policy evaluation**: use the state value function or state-action value function to evalute the policy function\n",
        "  2. **policy update**: use greedy policy or $ϵ$-greed policy\n",
        "\n",
        "In value-based methods, instead of learning a policy function, we learn a value function (that outputs the value of a state or a state-action pair) that maps a state to the expected value of being at that state.\n",
        "Given this value function, our policy will take an action.\n",
        "\n",
        "**The value of a state**：the expected discounted return the agent can get if it starts at that state and then acts according to our policy.\n",
        "(“Act according to our policy” just means that our policy is “going to the state with the highest value”.)\n",
        "\n",
        "## Two types of value-based methods:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_cv3csK_WBQc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tpqGoGcuXKVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyldtQ0fWZYL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}