{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1+z9lCcvbq4JPWWSSxzyz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelwfc/notes/blob/main/rl_notes/rl_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The big picture\n",
        "\n",
        "The idea behind Reinforcement Learning is that an agent (an AI) will learn from the environment by interacting with it (through trial and error) and receiving rewards (negative or positive) as feedback for performing actions.\n",
        "\n",
        "Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.\n",
        "## Teminology\n",
        "Term  | Explain | Formular(Category)\n",
        "--------|-----------|-------------\n",
        "state  | \n",
        "actioin |\n",
        "reward | a scalar feedback signal(random variable) which indicate how well the agent is doning at step t| $R_t$\n",
        "return | cumulative future reward(random variable) | $G_t=R_{t+1}+γR_{t+2}+γ^2R_{t+3}+...= R_{t+1}+γG_{t+1}$\n",
        "policy| agent's behave function, mappiing from state to action | Deterministic policy ($π(s)$) vs stochastive policy ($π(a|s):P[A_t=a|S_t=s]$)\n",
        "value fuction| how good is each state or action | \n",
        "state-value function| the mean of all possible trajectory return  |$V_{π}(s)=E_π[G_t|S_t=s]$ |\n",
        "action-value function| |$q_{π}(s,a)=E_π[G_t|S_t=s,A_t=a]$ |\n",
        "model| agenet's representaion of the enviroment \n",
        "enviroment|\n",
        "\n",
        "\n",
        "\n",
        "## **The reward hypothesis**: the central idea of Reinforcement Learning\n",
        "\n",
        "⇒ Why is the goal of the agent to **maximize the expected cumulative return**?\n",
        "reward can be interpreted as a **Human-Maichine Interface** with which we can guild the agent to behave as what we expect.\n",
        "Because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected return (expected cumulative reward).\n",
        "\n",
        "That’s why in Reinforcement Learning, to have the best behavior, we aim to learn to take actions that maximize the expected cumulative reward.\n",
        "\n",
        "**Rewards and the discounting**：The reward is fundamental in RL because it’s the only feedback for the agent. Thanks to it, our agent knows if the action taken was good or not.\n",
        "\n",
        "## The Policy π: the agent’s brain\n",
        "\n",
        "The Policy π is the brain of our Agent, it’s the function that tells us what action to take given the state we are in. So it defines the agent’s behavior at a given time.\n",
        "\n",
        "\n",
        "**the goal of an RL agent** is to have **an optimal policy π***：\n",
        "This Policy is the function we want to learn, our goal is to find the optimal policy π*, the policy that maximizes expected return when the agent acts according to it. We find this π* through training.\n",
        "\n",
        "\n",
        "There are two approaches to train our agent to find this optimal policy π*:\n",
        "\n",
        "\n",
        "# Two main approaches for solving RL problems\n",
        "## Policy-Based Methods\n",
        "  \n",
        "Directly, by teaching the agent to learn which action to take, given the current state. we learn a policy function directly.\n",
        "This function will define a mapping from each state to the best corresponding action. \n",
        "\n",
        "Alternatively, it could define a probability distribution over the set of possible actions at that state.\n",
        "\n",
        "\n",
        "## value-based methods\n",
        "  \n",
        "Indirectly, teach the agent to learn which state is more valuable and then take the action that leads to the more valuable states.\n",
        "  1. **policy evaluation**: use the state value function or state-action value function to evalute the policy function\n",
        "  2. **policy update**: use greedy policy or $ϵ$-greed policy\n",
        "\n",
        "In value-based methods, instead of learning a policy function, we learn a value function (that outputs the value of a state or a state-action pair) that maps a state to the expected value of being at that state.\n",
        "Given this value function, our policy will take an action.\n",
        "\n",
        "**The value of a state**：the expected discounted return the agent can get if it starts at that state and then acts according to our policy.\n",
        "(“Act according to our policy” just means that our policy is “going to the state with the highest value”.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Markove Decision Process\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_cv3csK_WBQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Bellman Equation: simplify our value estimation\n",
        "the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, which is a long process, we calculate the value as the **mean** of **immediate reward** + **the discounted value of the state that follows**.  \n",
        "$V_{π}(s)=E_π[G_t|S_t=s]$  \n",
        "$V_{π}(s)=E_π[R_{t+1}+γR_{t+2}+γ^2R_{t+3}+...|S_t=s]$  \n",
        "$V_{π}(s)=E_π[R_{t+1}+γG_{t+1}|S_t=s]$  \n",
        "$V_{π}(s)=E_π[R_{t+1}|S_t=s]+γE_π[G_{t+1}|S_t=s]$  \n",
        "$V_{π}(s)=E_π[R_{t+1}|S_t=s]+γ\\sum_{s'}p(s'|s)E_π[G_{t+1}|S_t=s,S_{t+1}=s']$  \n",
        "because of Markove propery of \n",
        "$V_{π}(s)=E_π[R_{t+1}|S_t=s]+γ\\sum_{s'}p(s'|s)E_π[G_{t+1}|S_{t+1}=s']$  \n",
        "$V_{π}(s)=E_π[R_{t+1}|S_t=s]+γ\\sum_{s'}p(s'|s)V_{π}(s')$  \n",
        "$V_{π}(s)=E_π[R_{t+1}|S_t=s]+γE_π[V_{\\pi}(s')|S_t=s]$\n",
        "\n",
        "-----\n",
        "\n",
        "$q_{π}(s,a)=E_π[G_t|S_t=s,A_t=a]$  \n",
        "$q_{π}(s,a)=E_π[R_{t+1}+γG_{t+1}|S_t=s,A_t=a]$  \n",
        "$q_{π}(s,a)=E_π[R_{t+1}|S_t=s,A_t=a]+γE_π[G_{t+1}|S_t=s,A_t=a]$  \n",
        "$q_{π}(s,a)=E_π[R_{t+1}|S_t=s,A_t=a]+γ\\sum_{s'}p(s'|s,a)E_π[G_{t+1}|S_t=s,A_t=a,S_{t+1}=s']$ \n",
        "because of Markove propery\n",
        "$q_{π}(s,a)=E_π[R_{t+1}|S_t=s,A_t=a]+γ\\sum_{s'}p(s'|s,a)E_π[G_{t+1}|S_{t+1}=s']$  \n",
        "$q_{π}(s,a)=E_π[R_{t+1}|S_t=s,A_t=a]+γ\\sum_{s'}p(s'|s,a)\\sum_{a'}π(a'|s')E_π[G_{t+1}|S_{t+1}=s',A_{t+1}=a']$  \n",
        "$q_{π}(s,a)=E_π[R_{t+1}|S_t=s,A_t=a]+γ\\sum_{s'}p(s'|s,a)\\sum_{a'}π(a'|s')q_{π}(s',a')]$\n",
        "$q_{π}(s,a)=E_π[R_{t+1}|S_t=s,A_t=a]+γE_π[q_{π}(s',a')||S_t=s,A_t=a]$"
      ],
      "metadata": {
        "id": "3XKMegfzsxjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monte Carlo vs Temporal Difference Learning\n",
        "\n",
        "Monte Carlo uses an entire episode of experience before learning. On the other hand, Temporal Difference uses only a step to learn.\n"
      ],
      "metadata": {
        "id": "r55IJ3_Ns9iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tpqGoGcuXKVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyldtQ0fWZYL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}