{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6/fZCSrpLN4yDBESUUhnc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelwfc/notes/blob/main/rl_notes/rl_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The big picture\n",
        "\n",
        "The idea behind Reinforcement Learning is that an agent (an AI) will learn from the environment by interacting with it (through trial and error) and receiving rewards (negative or positive) as feedback for performing actions.\n",
        "\n",
        "Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.\n",
        "## Teminology\n",
        "Term  | Explain | Formular(Category)\n",
        "--------|-----------|-------------\n",
        "state  | \n",
        "actioin |\n",
        "reward | a scalar feedback signal(random variable) which indicate how well the agent is doning at step t| $R_t$\n",
        "return | cumulative future reward(random variable) | $G_t=R_{t+1}+γR_{t+2}+γ^2R_{t+3}+...= R_{t+1}+γG_{t+1}$\n",
        "policy| agent's behave function, mappiing from state to action | Deterministic policy ($π(s)$) vs stochastive policy ($π(a|s):P[A_t=a|S_t=s]$)\n",
        "value fuction| how good is each state or action | \n",
        "state-value function| the mean of all possible trajectory return  |$V_{π}(s)=E_π[G_t|S_t=s]$ |\n",
        "action-value function| |$q_{π}(s,a)=E_π[G_t|S_t=s,A_t=a]$ |\n",
        "model| agenet's representaion of the enviroment \n",
        "enviroment|\n",
        "\n",
        "\n",
        "\n",
        "## **The reward hypothesis**: the central idea of Reinforcement Learning\n",
        "\n",
        "⇒ Why is the goal of the agent to **maximize the expected cumulative return**?\n",
        "reward can be interpreted as a **Human-Maichine Interface** with which we can guild the agent to behave as what we expect.\n",
        "Because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected return (expected cumulative reward).\n",
        "\n",
        "That’s why in Reinforcement Learning, to have the best behavior, we aim to learn to take actions that maximize the expected cumulative reward.\n",
        "\n",
        "**Rewards and the discounting**：The reward is fundamental in RL because it’s the only feedback for the agent. Thanks to it, our agent knows if the action taken was good or not.\n",
        "\n",
        "## The Policy π: the agent’s brain\n",
        "\n",
        "The Policy π is the brain of our Agent, it’s the function that tells us what action to take given the state we are in. So it defines the agent’s behavior at a given time.\n",
        "\n",
        "\n",
        "**the goal of an RL agent** is to have **an optimal policy $π^*$**：\n",
        "This Policy is the function we want to learn, our goal is to find the optimal policy π*, the policy that maximizes expected return when the agent acts according to it. We find this π* through training.\n",
        "\n"
      ],
      "metadata": {
        "id": "_cv3csK_WBQc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YaN3JovvbOFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Markove Decision Process"
      ],
      "metadata": {
        "id": "WRdPv6Ym6PaH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BuxDOpN96RIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Bellman Equation: simplify our value estimation\n",
        "the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, which is a long process, we calculate the value as the **mean** of **immediate reward** + **the discounted value of the state that follows**.  \n",
        "$V_{π}(s)=E_π[G_t|S_t=s]$  \n",
        "$V_{π}(s)=E_π[R_{t+1}+γR_{t+2}+γ^2R_{t+3}+...|S_t=s]$  \n",
        "$V_{π}(s)=E_π[R_{t+1}+γG_{t+1}|S_t=s]$  \n",
        "$V_{π}(s)=E_π[R_{t+1}|S_t=s]+γE_π[G_{t+1}|S_t=s]$  \n",
        "$V_{π}(s)=E_π[R_{t+1}|S_t=s]+γ\\sum_{s'}p(s'|s)E_π[G_{t+1}|S_t=s,S_{t+1}=s']$  \n",
        "because of Markove propery of   \n",
        "$V_{π}(s)=E_π[R_{t+1}|S_t=s]+γ\\sum_{s'}p(s'|s)E_π[G_{t+1}|S_{t+1}=s']$  \n",
        "$V_{π}(s)=E_π[R_{t+1}|S_t=s]+γ\\sum_{s'}p(s'|s)V_{π}(s')$  \n",
        "$V_{π}(s)=E_π[R_{t+1}|S_t=s]+γE_π[V_{\\pi}(s')|S_t=s]$\n",
        "\n",
        "-----\n",
        "$V_{π}(s)=\\sum_{a}π(a|s)E_π[R_{t+1}|S_t=s,A_t=a]+γ\\sum_{s'}p(s'|s)V_{π}(s')$    \n",
        "$V_{π}(s)=\\sum_{a}π(a|s)\\sum_rrp(R_{t+1}=r|S_t=s,A_t=a)+γ\\sum_{s'}\\sum_aπ(a|s)p(s'|s,a)V_{\\pi}(s')$    \n",
        "$V_{π}(s)=\\sum_{a}π(a|s)\\sum_rr\\sum_{s'}p(S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a)+γ\\sum_{s'}\\sum_aπ(a|s)\\sum_rp(S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a)V_{\\pi}(s')$    \n",
        "$V_{π}(s)=\\sum_{a}π(a|s)\\sum_{s',r}p(s',r|s,a)[r+γV_{\\pi}(s')]$ \n",
        "\n",
        "-----\n",
        "-----\n",
        "$V_{π}(s)=E_π[G_t|S_t=s]$  \n",
        "$V_{π}(s)=\\sum_a\\pi(a|s) E_π[G_t|S_t=s,A_t=a]$  \n",
        "$V_{π}(s)=\\sum_a\\pi(a|s) q_{π}(s,a)$\n",
        "\n",
        "\n",
        "-----\n",
        "-----\n",
        "\n",
        "\n",
        "$q_{π}(s,a)=E_π[G_t|S_t=s,A_t=a]$  \n",
        "$q_{π}(s,a)=E_π[R_{t+1}+γG_{t+1}|S_t=s,A_t=a]$  \n",
        "$q_{π}(s,a)=E_π[R_{t+1}|S_t=s,A_t=a]+γE_π[G_{t+1}|S_t=s,A_t=a]$  \n",
        "$q_{π}(s,a)=E_π[R_{t+1}|S_t=s,A_t=a]+γ\\sum_{s'}p(s'|s,a)E_π[G_{t+1}|S_t=s,A_t=a,S_{t+1}=s']$   \n",
        "because of Markove propery\n",
        "$q_{π}(s,a)=E_π[R_{t+1}|S_t=s,A_t=a]+γ\\sum_{s'}p(s'|s,a)E_π[G_{t+1}|S_{t+1}=s']$  \n",
        "$q_{π}(s,a)=E_π[R_{t+1}|S_t=s,A_t=a]+γ\\sum_{s'}p(s'|s,a)\\sum_{a'}π(a'|s')E_π[G_{t+1}|S_{t+1}=s',A_{t+1}=a']$  \n",
        "$q_{π}(s,a)=E_π[R_{t+1}|S_t=s,A_t=a]+γ\\sum_{s'}p(s'|s,a)\\sum_{a'}π(a'|s')q_{π}(s',a')]$\n",
        "$q_{π}(s,a)=E_π[R_{t+1}|S_t=s,A_t=a]+γE_π[q_{π}(s',a')|S_t=s,A_t=a]$\n",
        "\n",
        "----\n"
      ],
      "metadata": {
        "id": "3XKMegfzsxjR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYTjTCC58rQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Bellman Optimality Equation\n",
        "**optimal policy $π^*$**: A policy $π^*$ is optimal if   \n",
        "$V_{π^*}(s)>=V_{π}(s)$  \n",
        "for all s and for any other policy π.   \n",
        "\n",
        "**optimal state-value function $v^*（s）$**:   \n",
        "$v^*=max_πv_π(s)$  \n",
        "for all state s in S  \n",
        "\n",
        "**optimal action-value function $q^*(s,a)$**:  \n",
        "$q^*(s,a)=max_πq_π(s,a)$   \n",
        "for all state in S and all action a in A(s)  \n",
        "\n",
        "\n",
        "## Bellman optimality equation(elementwise form)\n",
        "from Bellman equation give policy $\\pi$  \n",
        "$V_{π}(s)=\\sum_{a}π(a|s)\\sum_rrp(r|s,a)+γ\\sum_{s'}\\sum_aπ(a|s)p(s'|s,a)V_{\\pi}(s')$   \n",
        "change it into a optimation problem:  \n",
        "$V(s)=max_{π}\\sum_{a}π(a|s)[\\sum_rrp(r|s,a)+γ\\sum_{s'}p(s'|s,a)V_{\\pi}(s')]$   \n",
        "$V(s)=max_{π}\\sum_{a}π(a|s) q_{π}(s,a)$  \n",
        "considing that $\\sum_{a}π(a|s)=1$, we have  \n",
        "$V(s)=max_{π}\\sum_{a}π(a|s) q_{π}(s,a)=max_{a∈A(s)}q_{π}(s,a)$   \n",
        "where the optimality is achieved when   \n",
        "$π(a|s)= \\frac{1, a=a^*}{0, a!=a^*}$   \n",
        "where $a^*= argmax_{a∈A(s)}q_{π}(s,a)$\n",
        "\n",
        "## Bellman optimality equation (matrix-vector form):\n",
        "$v=max_{π}(r_π+γP_πv)$"
      ],
      "metadata": {
        "id": "noNHD-MvzU1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Programming\n",
        "**The key idea of DP** and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies."
      ],
      "metadata": {
        "id": "5F4PnxNCWiKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wDPtIj3CXKt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two approaches to train our agent to find this optimal policy π*:\n",
        "\n",
        "# Two main approaches for solving RL problems\n",
        "## value-based methods\n",
        "  \n",
        "Indirectly, teach the agent to learn which state is more valuable and then take the action that leads to the more valuable states.\n",
        "\n",
        "In value-based methods, instead of learning a policy function, we learn a value function (that outputs the value of a state or a state-action pair) that maps a state to the expected value of being at that state.\n",
        "Given this value function, our policy will take an action.\n",
        "\n",
        "**The value of a state**：the expected discounted return the agent can get if it starts at that state and then acts according to our policy.\n",
        "(“Act according to our policy” just means that our policy is “going to the state with the highest value”.)\n",
        "\n",
        "## Policy-Based Methods  \n",
        "Directly, by teaching the agent to learn which action to take, given the current state. we learn a policy function directly.\n",
        "This function will define a mapping from each state to the best corresponding action. \n",
        "\n",
        "Alternatively, it could define a probability distribution over the set of possible actions at that state.\n",
        "\n",
        "\n",
        "## Value iteration algorithm:\n",
        "$v_{k+1}=max_{π}(r_π+γP_πv_k)$\n",
        "1. **policy update**: \n",
        "$\\pi_{k+1} =argmax_{π}(r_π+γP_πv_k)$  \n",
        "2. **value update**:\n",
        "$v_{k+1}=r_{π_{k+1}}+γP_{π_{k+1}}v_k$\n",
        "\n",
        "## Policy iteration algorithm\n",
        "Given a random initial policy $π_0$,\n",
        "1. policy evaluation (PE)  \n",
        "This step is to calculate the state value of $π_k$:  \n",
        "$v_{π_{k}}=r_{π_{k}}+γP_{π_{k}}v_{π_{k}}$\n",
        "2. policy improvement (PI)  \n",
        "$\\pi_{k+1} =argmax_{π}(r_π+γP_πv_{π_{k}})$ \n",
        "\n",
        "## Truncated policy iteration algorithm"
      ],
      "metadata": {
        "id": "0pn_gZmhzbpb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ybFSEQpBBZZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HFmqH4yd11oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monte Carlo\n",
        "The key to understand the algorithm is to understand how to convert the\n",
        "policy iteration algorithm to be model-free.\n",
        "\n",
        "## The MC Basic algorithm\n",
        "Given a random initial policy $π_0$,\n",
        "1. policy evaluation (PE)  \n",
        "This step is to calculate the  action-state value of $π_k$: $q_{π_{k}}(s,a)$ for all (s,a)   \n",
        "Specically, for each action-state pair (s; a), run an inffite\n",
        "number of (or sufficiently many) episodes. The average of their returns\n",
        "is used to approximate $q_{π_{k}}(s,a)$.  \n",
        "$q_{π_{k}}(s,a)=E_π[G_t|S_t=s,A_t=a]$    \n",
        "Suppose we have a set of episodes and hence $g^{(j)}(s,a)$   \n",
        "$q_{π_{k}}(s,a)=E_π[G_t|S_t=s,A_t=a]≈\\frac{1}{N}∑g^{(j)}(s,a)$  \n",
        "(model-free algorithms)\n",
        "\n",
        "\n",
        "2. policy improvement (PI)  \n",
        "$\\pi_{k+1} =argmax_{π}(r_π+γP_πv_{π_{k}})$   \n",
        "The greedy optimal policy is   \n",
        "$\\pi_{k+1}(a^*_k|s) = 1$ where $a^*_k= argmax_aq_{π_{k}}(s,a)$\n",
        "\n",
        "\n",
        "## Monte Carlo vs Temporal Difference Learning\n",
        "\n",
        "Monte Carlo uses an entire episode of experience before learning. On the other hand, Temporal Difference uses only a step to learn.\n"
      ],
      "metadata": {
        "id": "r55IJ3_Ns9iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tpqGoGcuXKVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyldtQ0fWZYL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}